{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Descripción de la tarea:**\n",
        "\n",
        "Sabiendo que tienes acceso a La solución AWS MLOps [1] que es un Framework que te ayuda a optimizar y aplicar las mejores prácticas de arquitectura para la producción de modelos de aprendizaje automático (ML), se te pide investigar como data scientist y proponer los pasos a seguir para poner en producción el case base reasoning [2]. "
      ],
      "metadata": {
        "id": "rHdLb6IidvDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar esta tarea se toma como base el siguiente diagrama que proviene del punto [1].\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/aws-solutions/mlops-workload-orchestrator/main/source/architecture-option-1.png\" style=\"max-width:100%;\"/>\n",
        "\n",
        "\n",
        "Y además se toma el anexo números [2] que corresponde a al case reasoning que es una solución de Inteligencia Artificial para resolver problemas que ha dado muy buenos resultados. Sin embargo para poder implementar dicho algoritmo se necesita una muy buena solución que sea escalable y resistente a fallos; es por esta razón que se elige implementarla en AWS, MLops.\n",
        "\n",
        "La sintésis del Case Reasoining se presenta a continuación: \n",
        "\n",
        "<img src= \"https://raw.githubusercontent.com/levraines/Portfolio/master/Master_Artificial%20Intelligence/Data/Screen%20Shot%202022-09-08%20at%206.34.57%20PM.png\" style=\"max-width:100%;\"/>\n",
        "\n",
        "Y se explica a comtinuación: CBR es una técnica de aprendizaje automático basada en la resolución de nuevos problemas utilizando la experiencia, como lo hacen los humanos. La experiencia se representa como un caso de memoria que contiene casos previamente resueltos. \n",
        "\n",
        "El ciclo CBR se puede resumir en cuatro pasos: \n",
        "\n",
        "1. Recuperación de los más similares casos.\n",
        "2. Adaptación a esos casos para proponer una nuevo solución al nuevo entorno.\n",
        "3. Verificación de la validez de la solución propuesta.\n",
        "4. Finalmente almacenamiento después de un política de aprendizaje. \n",
        "\n",
        "La técnica CBR se podría resumir en dos grandes bloques según su funcionalidad: un clasificador y un sintetizador. Una de las ventajas clásicas de CBR es su\n",
        "la simplicidad del clasificador, siendo un clasificador de algoritmo de vecinos más cercanos (K-NN) una opción común. Este ventaja aparente puede conducir a problemas colaterales a nivel de memoria, a nivel de lentitud cuando el\n",
        "el volumen de los datos crece considerablemente ya la síntesis de los datos. El bloque de síntesis se encarga de adaptar la experiencia y salvar el nuevo problema. De ahí la importancia de contar con un servicio potente para poder poner en producción dicha técnica de aprendizaje automático.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mRvE-ZZ0V7JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ingesta:**\n",
        "\n",
        "Se puede observar que en el diagrama se tiene el punto número y dice: \"Opción 1, iniciar mediante una llamada de API\". \n",
        "\n",
        "\n",
        "1. Primer microservicio: Amazon API Gateway, que se puede utilizar en caso de que los datos que queremos utilizar vengan de una página web o estén alojados en alguna otra base de datos on premise. Este microservicio nos puede servir para hacer una conexión entre nuestra página web/base de datos on premise y poder conectarlo a AWS Cloud. Perfectamente yo como Data Scientist quiero utilizar datos que vienen desde una página web y aquí podemos dar un ejemplo muy sencillo, podemos recolectar datos que vienen de páginas de juegos en internet que nos exigen estar continuamente jugando y aprendiendo cómo podemos vencer al algoritmo detrás, o puede ser que yo tenga dichos datos de partidas con los éxitos y fracasos en una base de datos on premise. \n",
        "2. Segundo microservicio: AWS Lambda, este corresponde a un servicio de AWS que nos permite escalar y provisionar bajo demanda una serie de ejecuciones serverless, en este caso gracias a este servicio podemos automatizar el proceso de llamada de datos que se hizo en el punto 1 (establecer la conexión con el API) y después alojarlos dentro de la nube para su posterior utilización. \n",
        "3. Tercer microservicio: Amazon S3, que es un datalake que nos permite almacenar todo tipo de datos, ya sean estructurados, no estructurados o semiestructurados y los podemos alojar ahora sí, dentro de la nube, entonces ya tenemos la primera fase del proceso completa.\n",
        "\n",
        "Resumen: primero crear una puerta de conexión entre el lugar donde están los datos que se van a utilizar en el case base reasoning con el API, luego utilizar una función Lambda que es automática y será la responsable de arrastrar dichos datos al AWS S3 para poder guardarlos dentro del cloud y utizarlos después. \n",
        "\n",
        "Esta primera fase corresponde a la obtención de los datos y puede tener una variante, como la que se observa en la opción 2: \"Inicializar por un Git commitment\", aquí simplemente se opta por iniciar el proceso mediante una línea de código pre definida y que está contenida en alguna de las versiones de Git (en nuestro caso de Github, Git, Gitlab) la que hará una ejecución automática a un microservicio llamado \"AWS CodeBuild\" que es un servicio de integración continua completamente administrado que compila código fuente y luego mediante la función lambda va a activar el Git commit, buscará los datos de dónde lo requiramos y finalmente los guardará en el S3.\n",
        "\n",
        "Como podemos ver este proceso es completamente automático y se presta para una integración continua y en la que nos garantizamos que siempre habrán datos que se están obteniendo y se están enviando para AWS."
      ],
      "metadata": {
        "id": "OovnVAHlYSHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Procesamiento y entrenamiento:**\n",
        "\n",
        "En nuestra segunda fase tenemos:\n",
        "\n",
        "\n",
        "\n",
        "1. Un orquestador es una pieza de software (middleware) que permite integrar servicios provenientes de diversas fuentes, y proveer información de forma síncrona o asíncrona, a través del uso de web services, colas, HTML, Bases de datos, correo, archivos, entre otras fuentes y destinos (como las que mencionamos en el apartado anterior). \n",
        "2. Amazon ECR: que es un servicio de registro de imágenes de contenedor administrado por AWS y con los que podemos acceder a los contenedores especificos que tengamos contratados para procesar nuestros datos, en este caso con los contenedores, nosotros como data scientist, nos garantizamos de que siempre tengamos los servicios y paquetes necesarios para correr nuestros algoritmos y que no hay problemas de compatibilidad entre nuestras versiones, además gracias a este servicio podemos aumentar o disminuir la cantidad de imágenes que tenemos y su capacidad de ejecución y procesamiento de datos (entre más datos más tiempo de ejecución y viceversa) por lo que, podemos facilamente como Data Scientist evaluar si decidimos alimentar nuestro modelo con datos de un día, una semana, un mes o incluso histórico, todo dependerá de cuánto estamos dispuestos a pagar. \n",
        "3. Amazon SageMaker Model Registry: este microservicio nos sirve para registrar modelos para producción, asociar metadatos y construir modelos en un flujo de CI-CD, en este caso podemos registrar nuestro modelo para utilizar los datos que obtuvimos en el paso número 1. \n",
        "4. Amazon S3 Model Artifact & Data: aquí está nuestro modelo pre-registrado, que perfectamente puede ser algún algoritmo estándar de clasificación de la nuve de AWS o podemos usar el algoritmo estándar del base case reasoning que es el KNN y en este caso dicho algoritmo está siendo entrenado mediante la ingesta continua de datos desde S3 que ya denotamos. "
      ],
      "metadata": {
        "id": "lW417HCBZ2v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Puesta en producción:**\n",
        "\n",
        "1. Con Cloud Formation puedo empezar a crear el pipeline necesario para trasladar mi case base reasoning a producción, es decir, encontrar la solución más óptima al problema que tengo basados en los casos que están en la base de datos S3, así es como empiezo a desencadenar la solución y la transmito al siguiente microservicio.\n",
        "2. AWS CodePipeline que es un servicio de entrega continua completamente administrado que permite automatizar canalizaciones de lanzamiento para lograr actualizaciones de infraestructura y aplicaciones rápidas y fiables, en este caso yo puedo tener en tiempo real y de forma casi inmediata resultados de mi case base reasoning y mediante este microservicio puedo saber qué solución encontró con los datos que le proporcioné. "
      ],
      "metadata": {
        "id": "tKzBisEiuLQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Entrega Continua y Evaluación:**\n",
        "\n",
        "Finalmente cuanto con los microservicios:\n",
        "\n",
        "1. Amazon SageMaker Model: en donde tengo todos los parámetros que definí de mi modelo y puedo estar revisando los parámetros que tengo configurados con los que se están realizando las predicciones. En este microservicio ya mi modelo se encuentra escalado y puesto en producción, es decir, está evaluando continuamente con los datos que se están ingestando de S3.\n",
        "2. Amazon SageMaker Endpoint: microservicio que es la \"llave\" de nuestro modelo y que permite hacer las predicciones en el ambiente final de producción. \n",
        "3. AWS API Gateway: ahora bien, ya tenemos nuestro modelo configurado, entrenado y haciendo producciones con los datos sumistrados de AWS S3, es momento de empotrar nuestro modelo en la página web de la cual obtuvimos nuestros datos de juego para ver cómo se comporta nuestro modelo con datos que el modelo no conoce preavimente, es decir, datos de la vida real del azar. \n",
        "4. Usuario: con todo lo anteriormente mencionado, yo como Data scientist puedo poner mi modelo a competir contra el algoritmo de la página de internet para ver qué tan bueno es aprendiendo de los errores y mejorando sus parámetros para poder ganarle a la página web y con esas victorias poder guardarlas dentro de S3 para que pueda tomar mejores decisiones cada vez a que se enfrente a un nuevo caso de solución y evaluar qué tan buenos están siendo mis resultados, en caso de que no lo sean puedo volver a empezar el proceso."
      ],
      "metadata": {
        "id": "5JYVTtZovcfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al final el proceso se ve de la siguiente forma:\n",
        "\n",
        "<img src= \"https://raw.githubusercontent.com/levraines/Portfolio/master/Master_Artificial%20Intelligence/Data/Screen%20Shot%202022-09-08%20at%207.30.08%20PM.png\" style=\"max-width:100%;\"/>\n",
        "\n",
        "Como se puede ver en el diagrama que traté de hacer, hay una integración y entrega de valor constante, ya que si mis resultados no son tan buenos para resolver el problema y obtener el mejor resultado vuelvo al paso número 1, es decir, volver a obtener datos de la página web para seguir entrenando mi modelo y que siga aprendiendo, pero si más bien mi modelo encuentra buenas soluciones, esos datos específicos los guardo para poder extrapolarlos a futuras soluciones y poder resolver el problema de forma más rápida y mejor. \n"
      ],
      "metadata": {
        "id": "HLnyHXL9x615"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones:**\n",
        "\n",
        "1. Gracias a AWS MLops podemos diseñar pipelines de modelos de ML que entreguen constante valor al usuario mediante soluciones escalables y que puedan ser automatizadas de forma rápida y segura. \n",
        "2. El proceso siempre será transparante, ya que siempre se busca por una mejora en los resultados, en el caso de no tener una mejora se garantiza que es un proceso que volverá a buscar datos y encontrar soluciones y en caso de que encuentre las mejores soluciones las guardará para poder extrapolarlas a casos futuros. \n",
        "3. Existen muchos microservicios pero no todos deben usarse, depende de cada caso de negocio y a qué estamos enfocados.\n",
        "4. Para el caso de case base reasoning es un algoritmo que se centra en aprender como los humanos y encontrar la mejor solución y en caso de no hacerlo, guarda todo lo que aprende y genera nuevos casos para volver a tomar decisiones cada vez con mayor precisión y rapidez. \n",
        "5. La puesta en producción es algo relativamente fácil de lograr en Cloud, pero es una solución altamente costosa que necesita muchísima optimización sino los costos de operación pueden ser muy costosos para la empresa. "
      ],
      "metadata": {
        "id": "ssE-2BfV4Bin"
      }
    }
  ]
}